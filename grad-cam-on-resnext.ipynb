{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":16880,"databundleVersionId":858837,"sourceType":"competition"},{"sourceId":888125,"sourceType":"datasetVersion","datasetId":458848},{"sourceId":904207,"sourceType":"datasetVersion","datasetId":484608},{"sourceId":1008856,"sourceType":"datasetVersion","datasetId":492598}],"dockerImageVersionId":29845,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install grad-cam","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:25.211429Z","iopub.execute_input":"2023-11-29T20:30:25.211748Z","iopub.status.idle":"2023-11-29T20:30:31.288383Z","shell.execute_reply.started":"2023-11-29T20:30:25.211704Z","shell.execute_reply":"2023-11-29T20:30:31.287499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading the required libraries\nimport os, sys, time\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nimport skimage.transform\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models, transforms\nfrom torch.autograd import Variable\nfrom torch import topk\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom matplotlib.pyplot import imshow\nfrom tqdm.notebook import tqdm\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-29T20:30:31.291094Z","iopub.execute_input":"2023-11-29T20:30:31.291430Z","iopub.status.idle":"2023-11-29T20:30:31.306081Z","shell.execute_reply.started":"2023-11-29T20:30:31.291368Z","shell.execute_reply":"2023-11-29T20:30:31.305092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read the test videos \ntest_dir = \"/kaggle/input/deepfake-detection-challenge/test_videos/\"\n\ntest_videos = sorted([x for x in os.listdir(test_dir) if x[-4:] == \".mp4\"])\nlen(test_videos)\n\n#Load the test labels -- this was created by mapping the metadata file with the test file names\ntest_labels = pd.read_csv('/kaggle/input/sample-face-crop/test_video_labels.csv')\ntest_labels.head()","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2023-11-29T20:30:31.307535Z","iopub.execute_input":"2023-11-29T20:30:31.307871Z","iopub.status.idle":"2023-11-29T20:30:31.334112Z","shell.execute_reply.started":"2023-11-29T20:30:31.307814Z","shell.execute_reply":"2023-11-29T20:30:31.333386Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels['label'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.335348Z","iopub.execute_input":"2023-11-29T20:30:31.335591Z","iopub.status.idle":"2023-11-29T20:30:31.343956Z","shell.execute_reply.started":"2023-11-29T20:30:31.335532Z","shell.execute_reply":"2023-11-29T20:30:31.343028Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Confirm the env variables\nprint(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\nprint(\"cuDNN version:\", torch.backends.cudnn.version())","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.347352Z","iopub.execute_input":"2023-11-29T20:30:31.347673Z","iopub.status.idle":"2023-11-29T20:30:31.355560Z","shell.execute_reply.started":"2023-11-29T20:30:31.347612Z","shell.execute_reply":"2023-11-29T20:30:31.354825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check if GPU is available\ngpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.359624Z","iopub.execute_input":"2023-11-29T20:30:31.359932Z","iopub.status.idle":"2023-11-29T20:30:31.366284Z","shell.execute_reply.started":"2023-11-29T20:30:31.359885Z","shell.execute_reply":"2023-11-29T20:30:31.365544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Attach the required libraries to the system path\n# This have a few helper functions & path to the pre-trained model\n\nimport sys\nsys.path.insert(0, \"/kaggle/input/blazeface-pytorch\")\nsys.path.insert(0, \"/kaggle/input/deepfakes-inference-demo\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.367421Z","iopub.execute_input":"2023-11-29T20:30:31.367629Z","iopub.status.idle":"2023-11-29T20:30:31.375555Z","shell.execute_reply.started":"2023-11-29T20:30:31.367592Z","shell.execute_reply":"2023-11-29T20:30:31.374894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initalize blazeface \n\nfrom blazeface import BlazeFace\nfacedet = BlazeFace().to(gpu)\nfacedet.load_weights(\"/kaggle/input/blazeface-pytorch/blazeface.pth\")\nfacedet.load_anchors(\"/kaggle/input/blazeface-pytorch/anchors.npy\")\n_ = facedet.train(False)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.377043Z","iopub.execute_input":"2023-11-29T20:30:31.377385Z","iopub.status.idle":"2023-11-29T20:30:31.409720Z","shell.execute_reply.started":"2023-11-29T20:30:31.377330Z","shell.execute_reply":"2023-11-29T20:30:31.409134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from helpers.read_video_1 import VideoReader\nfrom helpers.face_extract_1 import FaceExtractor\n\nframes_per_video = 16\n\nvideo_reader = VideoReader()\nvideo_read_fn = lambda x: video_reader.read_frames(x, num_frames=frames_per_video)\nface_extractor = FaceExtractor(video_read_fn, facedet)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.411186Z","iopub.execute_input":"2023-11-29T20:30:31.411494Z","iopub.status.idle":"2023-11-29T20:30:31.418772Z","shell.execute_reply.started":"2023-11-29T20:30:31.411436Z","shell.execute_reply":"2023-11-29T20:30:31.417838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_size = 224 # Define the input size of the image\n\n# Define the normalizing functions with ImageNet parameters \nfrom torchvision.transforms import Normalize\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize_transform = Normalize(mean, std)\n\n# Define some helper functions for re-sizing image & making them into perfect squares\ndef isotropically_resize_image(img, size, resample=cv2.INTER_AREA):\n    h, w = img.shape[:2]\n    if w > h:\n        h = h * size // w\n        w = size\n    else:\n        w = w * size // h\n        h = size\n\n    resized = cv2.resize(img, (w, h), interpolation=resample)\n    return resized\n\n\ndef make_square_image(img):\n    h, w = img.shape[:2]\n    size = max(h, w)\n    t = 0\n    b = size - h\n    l = 0\n    r = size - w\n    return cv2.copyMakeBorder(img, t, b, l, r, cv2.BORDER_CONSTANT, value=0)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.420536Z","iopub.execute_input":"2023-11-29T20:30:31.420870Z","iopub.status.idle":"2023-11-29T20:30:31.433155Z","shell.execute_reply.started":"2023-11-29T20:30:31.420812Z","shell.execute_reply":"2023-11-29T20:30:31.432257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the ResNext Model with the given blocks & load the checkpoint\n\nimport torch.nn as nn\nimport torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n        \n        self.fc = nn.Linear(2048, 1)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.434564Z","iopub.execute_input":"2023-11-29T20:30:31.434916Z","iopub.status.idle":"2023-11-29T20:30:31.446213Z","shell.execute_reply.started":"2023-11-29T20:30:31.434871Z","shell.execute_reply":"2023-11-29T20:30:31.445519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Load the checkpoint & update the model for prediction\ncheckpoint = torch.load(\"/kaggle/input/deepfakes-inference-demo/resnext.pth\", map_location=gpu)\n\nmodel = MyResNeXt().to(gpu)\nmodel.load_state_dict(checkpoint)\n_ = model.eval()\n\ndel checkpoint","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.447683Z","iopub.execute_input":"2023-11-29T20:30:31.447968Z","iopub.status.idle":"2023-11-29T20:30:31.922239Z","shell.execute_reply.started":"2023-11-29T20:30:31.447925Z","shell.execute_reply":"2023-11-29T20:30:31.921560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#model.layer4 <<- You can use this to inspect the 4th layer or any other layers","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.923648Z","iopub.execute_input":"2023-11-29T20:30:31.923924Z","iopub.status.idle":"2023-11-29T20:30:31.927597Z","shell.execute_reply.started":"2023-11-29T20:30:31.923880Z","shell.execute_reply":"2023-11-29T20:30:31.926765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = 'REAL'\nwhile y == 'REAL':\n    sample_video= random.choice(test_videos) # Select a random test video \n    video_path = os.path.join(test_dir, sample_video)\n    y = test_labels[test_labels['processedVideo'] == sample_video]['label'].values[0]\nprint(\"Selected Video: \", sample_video)\nprint(\"True Value: \",y)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.928860Z","iopub.execute_input":"2023-11-29T20:30:31.929123Z","iopub.status.idle":"2023-11-29T20:30:31.943978Z","shell.execute_reply.started":"2023-11-29T20:30:31.929083Z","shell.execute_reply":"2023-11-29T20:30:31.943223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 16 # Extract faces from 16 frames in the video\nfaces = face_extractor.process_video(video_path)\nprint(\"No. of frames extracted: \", len(faces))\nprint(\"Keys in the extracted info: \", faces[0].keys())\ntry:\n    print(\"Shape of extracted face_crop: \", faces[0]['faces'][0].shape) # multiple faces can be captured. In this set only a single face is detected\n    print(\"Scores of the face crop: \", faces[0]['scores'][0])\nexcept:\n    print(\"=====================================\")\n    print(\"No faces detected! Please run again.\")","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:31.945154Z","iopub.execute_input":"2023-11-29T20:30:31.945463Z","iopub.status.idle":"2023-11-29T20:30:33.633760Z","shell.execute_reply.started":"2023-11-29T20:30:31.945395Z","shell.execute_reply":"2023-11-29T20:30:33.632646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Only look at one face per frame. This removes multiple faces from each frame, keeping only the best face\nface_extractor.keep_only_best_face(faces)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.635679Z","iopub.execute_input":"2023-11-29T20:30:33.636059Z","iopub.status.idle":"2023-11-29T20:30:33.640751Z","shell.execute_reply.started":"2023-11-29T20:30:33.635995Z","shell.execute_reply":"2023-11-29T20:30:33.639646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def preprocessTensor(sample_face):\n    \"\"\"\n    param imageArray: face crop passed to the function\n    return imageTensor(x), resized_face: The processed tensor and resized_face\n    \n    The following activities are performed:\n    1# resizing the image, via zero padding\n    2# make the crop into a square\n    3# Convert to a tensor and load to the gpu\n    4# Permutate the axis so that the channel is at zero index\n    5# Normalize & add a new dimension in zero index\n    \"\"\"\n    resized_face = isotropically_resize_image(sample_face, input_size)\n    resized_face = make_square_image(resized_face)\n    \n    x = torch.tensor(resized_face, device=gpu).float()\n    x = x.permute(( 2, 0, 1))\n    x = normalize_transform(x / 255.)\n    x = x.unsqueeze(0)\n    \n    return x, resized_face","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.642012Z","iopub.execute_input":"2023-11-29T20:30:33.642271Z","iopub.status.idle":"2023-11-29T20:30:33.652268Z","shell.execute_reply.started":"2023-11-29T20:30:33.642212Z","shell.execute_reply":"2023-11-29T20:30:33.651370Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resize to the model's required input size.\n# We keep the aspect ratio intact and add zero --???? This could be a problem! \n# padding if necessary.  \n\nsample_face = faces[0]['faces'][0]\nx, resized_face = preprocessTensor(sample_face)\nprint(x.shape)\n\n# Make a prediction.\nwith torch.no_grad():\n    y_pred = model(x)\n    y_pred = torch.sigmoid(y_pred.squeeze())\n\nprint(\"Prediction: \", y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.653671Z","iopub.execute_input":"2023-11-29T20:30:33.653948Z","iopub.status.idle":"2023-11-29T20:30:33.684648Z","shell.execute_reply.started":"2023-11-29T20:30:33.653898Z","shell.execute_reply":"2023-11-29T20:30:33.683967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Resize to the model's required input size.\n# We keep the aspect ratio intact and add zero --???? This could be a problem! \n# padding if necessary.                    \nsample_face = faces[0]['faces'][0]\nresized_face = isotropically_resize_image(sample_face, input_size)\nresized_face = make_square_image(resized_face)\nresized_face.shape","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.686096Z","iopub.execute_input":"2023-11-29T20:30:33.686362Z","iopub.status.idle":"2023-11-29T20:30:33.693261Z","shell.execute_reply.started":"2023-11-29T20:30:33.686306Z","shell.execute_reply":"2023-11-29T20:30:33.692531Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"x = torch.tensor(resized_face, device=gpu).float()\nprint(x.shape)\n# Preprocess the images.\nx = x.permute(( 2, 0, 1))\nx = normalize_transform(x / 255.)\nx = x.unsqueeze(0)\nprint(x.shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.694608Z","iopub.execute_input":"2023-11-29T20:30:33.694934Z","iopub.status.idle":"2023-11-29T20:30:33.705707Z","shell.execute_reply.started":"2023-11-29T20:30:33.694889Z","shell.execute_reply":"2023-11-29T20:30:33.704796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Make a prediction.\nwith torch.no_grad():\n    y_pred = model(x)\n    y_pred = torch.sigmoid(y_pred.squeeze())\n\nprint(\"Prediction: \", y_pred)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.706974Z","iopub.execute_input":"2023-11-29T20:30:33.707270Z","iopub.status.idle":"2023-11-29T20:30:33.731646Z","shell.execute_reply.started":"2023-11-29T20:30:33.707226Z","shell.execute_reply":"2023-11-29T20:30:33.731027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SaveFeatures(): \n    features=None\n    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)  # attach the hook to the specified layer\n    def hook_fn(self, module, input, output): self.features = ((output.cpu()).data).numpy() # copy the activation features as an instance variable\n    def remove(self): self.hook.remove()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.733022Z","iopub.execute_input":"2023-11-29T20:30:33.733270Z","iopub.status.idle":"2023-11-29T20:30:33.739004Z","shell.execute_reply.started":"2023-11-29T20:30:33.733223Z","shell.execute_reply":"2023-11-29T20:30:33.738248Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_layer = model._modules.get('layer4') # Grab the final layer of the model\nactivated_features = SaveFeatures(final_layer) # attach the call back hook to the final layer of the model","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.740626Z","iopub.execute_input":"2023-11-29T20:30:33.740959Z","iopub.status.idle":"2023-11-29T20:30:33.748784Z","shell.execute_reply.started":"2023-11-29T20:30:33.740906Z","shell.execute_reply":"2023-11-29T20:30:33.748029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"prediction_var = Variable(x.cuda(), requires_grad=True) # Squeeze the  variable to add an additional dimension & then\nprediction_var.shape                                    # wrap it in a Variable which stores the grad_training weights","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.749883Z","iopub.execute_input":"2023-11-29T20:30:33.750091Z","iopub.status.idle":"2023-11-29T20:30:33.759894Z","shell.execute_reply.started":"2023-11-29T20:30:33.750056Z","shell.execute_reply":"2023-11-29T20:30:33.758993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = model(prediction_var)\ny_pred = torch.sigmoid(y_pred.squeeze())\n\nprint(\"Prediction: \", y_pred)\npred_probabilities = F.softmax(y_pred).data.squeeze() # Pass the predictions through a softmax layer to convert into probabilities for each class\nprint(\"Predicted Class: \", pred_probabilities)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.761127Z","iopub.execute_input":"2023-11-29T20:30:33.761606Z","iopub.status.idle":"2023-11-29T20:30:33.790654Z","shell.execute_reply.started":"2023-11-29T20:30:33.761373Z","shell.execute_reply":"2023-11-29T20:30:33.789994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getCAM(feature_conv, weight_fc, class_idx):\n    _, nc, h, w = feature_conv.shape\n    cam = weight_fc.dot(feature_conv.reshape((nc, h*w)))\n    cam = cam.reshape(h, w)\n    cam = cam - np.min(cam)\n    cam_img = cam / np.max(cam)\n    return [cam_img]\n\n#overlay = getCAM(activated_features.features, weight_softmax, 0. )","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.791881Z","iopub.execute_input":"2023-11-29T20:30:33.792094Z","iopub.status.idle":"2023-11-29T20:30:33.798515Z","shell.execute_reply.started":"2023-11-29T20:30:33.792054Z","shell.execute_reply":"2023-11-29T20:30:33.797659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_softmax_params = list(model._modules.get('fc').parameters()) # This gives a list of weights for the fully connected layers \nprint(len(weight_softmax_params))\nprint(weight_softmax_params[0].shape, weight_softmax_params[1].shape) # weghts for the last two layers\nweight_softmax = weight_softmax_params[0].cpu().data.numpy() # what does this do ??\nprint(weight_softmax.shape)\n#activated_features.remove()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.799850Z","iopub.execute_input":"2023-11-29T20:30:33.800146Z","iopub.status.idle":"2023-11-29T20:30:33.811973Z","shell.execute_reply.started":"2023-11-29T20:30:33.800092Z","shell.execute_reply":"2023-11-29T20:30:33.811223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cam_img = getCAM(activated_features.features, weight_softmax, pred_probabilities )\nprint(cam_img[0].shape)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.813225Z","iopub.execute_input":"2023-11-29T20:30:33.813447Z","iopub.status.idle":"2023-11-29T20:30:33.824572Z","shell.execute_reply.started":"2023-11-29T20:30:33.813408Z","shell.execute_reply":"2023-11-29T20:30:33.823406Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"imshow(cam_img[0], alpha=0.5, cmap='jet')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:33.826827Z","iopub.execute_input":"2023-11-29T20:30:33.827569Z","iopub.status.idle":"2023-11-29T20:30:34.067654Z","shell.execute_reply.started":"2023-11-29T20:30:33.827504Z","shell.execute_reply":"2023-11-29T20:30:34.066913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pytorch_grad_cam import GradCAM\nfrom pytorch_grad_cam.utils.model_targets import BinaryClassifierOutputTarget\nfrom pytorch_grad_cam.utils.image import show_cam_on_image\ntargets = [BinaryClassifierOutputTarget(1)]\ntarget_layers = [model.layer3[-1]]\ncam = GradCAM(model=model, target_layers=target_layers, use_cuda=False)\ngrayscale_cam = cam(input_tensor=prediction_var, targets=targets)\ngrayscale_cam = grayscale_cam[0, :]\nresized_face_norm = resized_face / 255\nvisualization = show_cam_on_image(resized_face_norm, grayscale_cam, use_rgb=True)\nimshow(visualization)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:34.069862Z","iopub.execute_input":"2023-11-29T20:30:34.070405Z","iopub.status.idle":"2023-11-29T20:30:34.487741Z","shell.execute_reply.started":"2023-11-29T20:30:34.070352Z","shell.execute_reply":"2023-11-29T20:30:34.486681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1,2, figsize=(10,10))\n\nax[0].imshow(resized_face)\nax[0].set_title(\"Video: \" + sample_video + \"Actual: \" + y )\nax[1].imshow(resized_face)\nax[1].imshow(skimage.transform.resize(cam_img[0], (resized_face.shape[0],resized_face.shape[1] )), alpha=0.25, cmap='jet')\ny_pred = str(y_pred.cpu().data.numpy())\nax[1].set_title(y_pred)\nfig.tight_layout()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:34.489532Z","iopub.execute_input":"2023-11-29T20:30:34.490142Z","iopub.status.idle":"2023-11-29T20:30:35.351798Z","shell.execute_reply.started":"2023-11-29T20:30:34.490075Z","shell.execute_reply":"2023-11-29T20:30:35.351204Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels['y_pred'] = ypred\n\ntest_labels['label_10'] = test_labels['label'].apply(lambda x: 1. if x == 'FAKE' else 0.) # Converting the labels to an ordinal value \ntest_labels['diff'] = abs(test_labels['label_10'] - test_labels['y_pred']) # Taking the naive absolute difference of the prediction to the actual ordinal value\ntest_labels.sort_values(by=['diff'], ascending=False, inplace=True) # Sorting the dataframe on difference gives us the samples with the most digression\ntest_labels.reset_index(drop=True, inplace=True)\ntest_labels.head() # LEts check the samples with the highest loss","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:35.355206Z","iopub.execute_input":"2023-11-29T20:30:35.355550Z","iopub.status.idle":"2023-11-29T20:30:35.384623Z","shell.execute_reply.started":"2023-11-29T20:30:35.355512Z","shell.execute_reply":"2023-11-29T20:30:35.383366Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels['diff'].describe()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:35.385944Z","iopub.status.idle":"2023-11-29T20:30:35.386519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_labels[test_labels['label_10'] == 0].head()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:35.387780Z","iopub.status.idle":"2023-11-29T20:30:35.388356Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Store the predictions. Since I don't want to keep running the above loop \ntest_labels.to_csv('test_labels.csv')","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:35.389596Z","iopub.status.idle":"2023-11-29T20:30:35.390250Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"idx = 16\nsample_video = test_labels.iloc[idx]['processedVideo']\nvideo_path = os.path.join(test_dir, sample_video)\ny = test_labels.iloc[idx]['label']\nprint(sample_video)\n\nbatch_size = 1 # Extract faces from 16 frames in the video\nfaces = face_extractor.process_video(video_path)\ntry:\n    sample_face = faces[0]['faces'][0]\n    x, resized_face = preprocessTensor(sample_face)\n    \n    final_layer = model._modules.get('layer4')\n    activated_features = SaveFeatures(final_layer)\n    \n    prediction_var = Variable(x.cuda(), requires_grad=True) \n    y_pred = model(prediction_var)\n    y_pred = torch.sigmoid(y_pred.squeeze())\n    \n    pred_probabilities = F.softmax(y_pred).data.squeeze()\n    weight_softmax_params = list(model._modules.get('fc').parameters())\n    \n    weight_softmax = weight_softmax_params[0].cpu().data.numpy() \n    \n    cam_img = getCAM(activated_features.features, weight_softmax, pred_probabilities )\n    \n    fig, ax = plt.subplots(1,2, figsize=(10,10))\n\n    ax[0].imshow(resized_face)\n    ax[0].set_title(\"Video: \" + sample_video + \" Actual: \" + y )\n    ax[1].imshow(resized_face)\n    ax[1].imshow(skimage.transform.resize(cam_img[0], (resized_face.shape[0],resized_face.shape[1] )), alpha=0.25, cmap='jet')\n    y_pred = \"PredProb:\" + str(y_pred.cpu().data.numpy()) + \" DIFF: \" + str(test_labels.iloc[idx]['diff'])\n    ax[1].set_title(y_pred)\n    fig.tight_layout()\nexcept:\n    print(\"Error processing file: \", sample_video)\n    ","metadata":{"execution":{"iopub.status.busy":"2023-11-29T20:30:35.391320Z","iopub.status.idle":"2023-11-29T20:30:35.391922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}